{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a8699e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.image import ImageSchema\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StructType, StructField, ArrayType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3432cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd659112",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa993cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b12c57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa2903c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = pyspark.SparkConf().setAppName('sba22177').setMaster('local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca804b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41cecc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session = SparkSession.builder \\\n",
    "      .master(\"local\") \\\n",
    "      .appName(\"sba22177_CA01\") \\\n",
    "      .config(\"spark.executor.memory\", \"6gb\") \\\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbad5142",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dowload the MNIST data\n",
    "(train_X, train_y), (test_X, test_y) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0da4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#examining dataset\n",
    "print(test_X.shape)\n",
    "print(train_X.shape)\n",
    "print(test_y.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43de635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing unique labels\n",
    "set(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc03631",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the image classes so not just numbers as above\n",
    "class_names = ['T-shirt/top','Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e665a330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature scaling colours from 0-255 to 0-1\n",
    "train_X = train_X/255.0\n",
    "test_X = test_X/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637405a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing the first 10 images\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(10):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_X[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[train_y[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1031b9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "train_images = train_images[:2000]\n",
    "train_labels = train_labels[:2000]\n",
    "\n",
    "#Reshape and normalize images\n",
    "train_images = train_images.reshape(-1, 28, 28, 1).astype(\"float32\") / 255\n",
    "test_images = test_images.reshape(-1, 28, 28, 1).astype(\"float32\") / 255\n",
    "\n",
    "#Convert labels to categorical\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca3bea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"image\", ArrayType(ArrayType(ArrayType(DoubleType()))), nullable=False),\n",
    "    StructField(\"label\", ArrayType(DoubleType()), nullable=False)\n",
    "])\n",
    "\n",
    "#Create Spark DataFrames\n",
    "train_data = [(train_images[i].tolist(), train_labels[i].tolist()) for i in range(len(train_images))]\n",
    "test_data = [(test_images[i].tolist(), test_labels[i].tolist()) for i in range(len(test_images))]\n",
    "\n",
    "train_df = spark.createDataFrame(train_data, schema=schema)\n",
    "test_df = spark.createDataFrame(test_data, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcf4203",
   "metadata": {},
   "source": [
    "## Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71ede23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating one hidden layer with 128 nodes and output layer with 10 nodes\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)), \n",
    "    tf.keras.layers.Dense(128, activation='relu'),    \n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce180abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#backpropagation\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65494899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "model.fit(train_X, train_y, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0612724d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating model\n",
    "loss, acc = model.evaluate(test_X, test_y, verbose = 1)\n",
    "print('\\\\nTest accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51127e95",
   "metadata": {},
   "source": [
    "## CNN_Model_One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2d03cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_one():\n",
    "    \n",
    "    #creating a sequential instance to add layers to model\n",
    "    cnn_model = Sequential()\n",
    "    \n",
    "    #first convolutional layer\n",
    "    cnn_model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(28,28,1)))\n",
    "    #first pooling layer\n",
    "    cnn_model.add(MaxPooling2D((2,2), strides = 2))\n",
    "    \n",
    "    #second convolutional layer\n",
    "    cnn_model.add(Conv2D(64, (3,3), padding='same', activation = 'relu'))\n",
    "    #second pooling layer\n",
    "    cnn_model.add(MaxPooling2D((2,2), strides = 2))\n",
    "    \n",
    "    #third convolutional layer\n",
    "    cnn_model.add(Conv2D(128, (3,3), padding='same', activation = 'relu'))\n",
    "    #third pooling layer\n",
    "    cnn_model.add(MaxPooling2D((2,2), strides = 2))\n",
    "    \n",
    "    #Flatten and connect layers\n",
    "    cnn_model.add(Flatten())\n",
    "    cnn_model.add(Dense(128, activation = 'relu'))\n",
    "    \n",
    "    #Fully connected layer of 10 to refelct 10 labels and softmax activation\n",
    "    cnn_model.add(Dense(10, activation = 'softmax'))\n",
    "    \n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de434aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_one = cnn_model_one()\n",
    "\n",
    "# set up model optimizer, loss function, and accuracy\n",
    "#adam - adaptive moment estimation\n",
    "cnn_model_one.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "cnn_model_one.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414c4112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkdl import KerasImageFileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd42f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Keras Image File Transformer for training\n",
    "keras_transformer = KerasImageFileTransformer(inputCol=\"image\", outputCol=\"prediction\", model=cnn_model_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccf0bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a pipeline for resizing and training\n",
    "pipeline = Pipeline(stages=[resizer, keras_transformer])\n",
    "\n",
    "# Train the model\n",
    "model_pipeline = pipeline.fit(train_df)\n",
    "\n",
    "# Evaluate the Model\n",
    "test_predictions = model_pipeline.transform(test_df)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(test_predictions)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Stop Spark Session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a2e53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "t0_one=time.time()\n",
    "train_model_one = cnn_model_one.fit(train_X, train_y, epochs=10, validation_split=0.33)\n",
    "print(\"Training time:\", time.time()-t0_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78e7565",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model\n",
    "loss, acc = cnn_model_one.evaluate(test_X, test_y, verbose = 1)\n",
    "print('\\nTest accuracy: ', acc)\n",
    "print('\\nTest loss: ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81e58f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the accuracy\n",
    "plt.plot(train_model_one.history['sparse_categorical_accuracy'])\n",
    "plt.plot(train_model_one.history['val_sparse_categorical_accuracy'])\n",
    "plt.title('Model Accuracy - CNN One')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9790ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the loss\n",
    "plt.plot(train_model_one.history['loss'])\n",
    "plt.plot(train_model_one.history['val_loss'])\n",
    "plt.title('Model Loss - CNN One')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3057263d",
   "metadata": {},
   "source": [
    "## CNN_Model_One - Epoch Increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c426f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "t0_one_epoch=time.time()\n",
    "train_model_one_epoch = cnn_model_one.fit(train_X, train_y, epochs=30, validation_split=0.33)\n",
    "print(\"Training time:\", time.time()-t0_one_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e412a43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model\n",
    "loss, acc = cnn_model_one.evaluate(test_X, test_y, verbose = 1)\n",
    "print('\\nTest accuracy: ', acc)\n",
    "print('\\nTest loss: ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eec06de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the accuracy\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.plot(train_model_one_epoch.history['sparse_categorical_accuracy'], label='Train')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('Train')\n",
    "ax1.tick_params(axis='y')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.plot(train_model_one_epoch.history['val_sparse_categorical_accuracy'], label='Validation', color='orange')\n",
    "ax2.set_ylabel('Validation')\n",
    "ax2.tick_params(axis='y')\n",
    "\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Model Accuracy - CNN One Epoch Increase')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d396f224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the loss\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.plot(train_model_one_epoch.history['loss'], label='Train')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('Train')\n",
    "ax1.tick_params(axis='y')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.plot(train_model_one_epoch.history['val_loss'], label='Validation', color='orange')\n",
    "ax2.set_ylabel('Validation')\n",
    "ax2.tick_params(axis='y')\n",
    "\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Model Loss - CNN One Epoch Increase')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72be3f4a",
   "metadata": {},
   "source": [
    "## CNN_Model_Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79a96cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_two():\n",
    "    \n",
    "    #creating a sequential instance to add layers to model\n",
    "    cnn_model_two = Sequential()\n",
    "    \n",
    "    #first convolutional layer\n",
    "    cnn_model_two.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(28,28,1)))\n",
    "    #first pooling layer\n",
    "    cnn_model_two.add(MaxPooling2D((2,2), strides = 2))\n",
    "    \n",
    "    #second convolutional layer\n",
    "    cnn_model_two.add(Conv2D(64, (3,3), padding='same', activation = 'relu'))\n",
    "    #second pooling layer\n",
    "    cnn_model_two.add(MaxPooling2D((2,2), strides = 2))\n",
    "    \n",
    "    #third convolutional layer\n",
    "    cnn_model_two.add(Conv2D(128, (3,3), padding='same', activation = 'relu'))\n",
    "    #third pooling layer\n",
    "    cnn_model_two.add(MaxPooling2D((2,2), strides = 2))\n",
    "    \n",
    "    #Flatten and connect layers\n",
    "    cnn_model_two.add(Flatten())\n",
    "    cnn_model_two.add(Dense(128, activation = 'relu'))\n",
    "    cnn_model_two.add(Dense(128, activation = 'relu'))\n",
    "    cnn_model_two.add(Dense(128, activation = 'relu'))\n",
    "    cnn_model_two.add(Dense(128, activation = 'relu'))\n",
    "    cnn_model_two.add(Dense(128, activation = 'relu'))\n",
    "    cnn_model_two.add(Dense(128, activation = 'relu'))\n",
    "    \n",
    "    #Fully connected layer of 10 to refelct 10 labels and softmax activation\n",
    "    cnn_model_two.add(Dense(10, activation = 'softmax'))\n",
    "    \n",
    "    return cnn_model_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2f91d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_two = cnn_model_two()\n",
    "\n",
    "# set up model optimizer, loss function, and accuracy\n",
    "#adam - adaptive moment estimation\n",
    "cnn_model_two.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "cnn_model_two.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b7d0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "t0_two=time.time()\n",
    "train_model_two = cnn_model_two.fit(train_X, train_y, epochs=10, validation_split=0.33)\n",
    "print(\"Training time:\", time.time()-t0_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46137026",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model\n",
    "loss, acc = cnn_model_two.evaluate(test_X, test_y, verbose = 1)\n",
    "print('\\nTest accuracy: ', acc)\n",
    "print('\\nTest loss: ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652ff5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the accuracy\n",
    "plt.plot(train_model_two.history['sparse_categorical_accuracy'])\n",
    "plt.plot(train_model_two.history['val_sparse_categorical_accuracy'])\n",
    "plt.title('Model Accuracy - CNN Two')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a42f356",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the loss\n",
    "plt.plot(train_model_two.history['loss'])\n",
    "plt.plot(train_model_two.history['val_loss'])\n",
    "plt.title('Model Loss - CNN Two')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fa1e9c",
   "metadata": {},
   "source": [
    "## CNN_MODEL_THREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ccdbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_three():\n",
    "    \n",
    "    #creating a sequential instance to add layers to model\n",
    "    cnn_model_three = Sequential()\n",
    "    \n",
    "    #first convolutional layer\n",
    "    cnn_model_three.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(28,28,1)))\n",
    "    #first pooling layer\n",
    "    cnn_model_three.add(MaxPooling2D((2,2), strides = 2))\n",
    "    #add dropout\n",
    "    model.add(Dropout(0.3)) \n",
    "    \n",
    "    #second convolutional layer\n",
    "    cnn_model_three.add(Conv2D(64, (3,3), padding='same', activation = 'relu'))\n",
    "    #second pooling layer\n",
    "    cnn_model_three.add(MaxPooling2D((2,2), strides = 2))\n",
    "    #add dropout\n",
    "    model.add(Dropout(0.3)) \n",
    "    \n",
    "    #third convolutional layer\n",
    "    cnn_model_three.add(Conv2D(128, (3,3), padding='same', activation = 'relu'))\n",
    "    #third pooling layer\n",
    "    cnn_model_three.add(MaxPooling2D((2,2), strides = 2))\n",
    "    #add dropout\n",
    "    model.add(Dropout(0.3)) \n",
    "    \n",
    "    #Flatten and connect layers\n",
    "    cnn_model_three.add(Flatten())\n",
    "    cnn_model_three.add(Dense(128, activation = 'relu'))\n",
    "    cnn_model_three.add(Dense(128, activation = 'relu'))\n",
    "    cnn_model_three.add(Dense(128, activation = 'relu'))\n",
    "    cnn_model_three.add(Dense(128, activation = 'relu'))\n",
    "    cnn_model_three.add(Dense(128, activation = 'relu'))\n",
    "    cnn_model_three.add(Dense(128, activation = 'relu'))\n",
    "    model.add(Dropout(0.3)) \n",
    "    \n",
    "    #Fully connected layer of 10 to refelct 10 labels and softmax activation\n",
    "    cnn_model_three.add(Dense(10, activation = 'softmax'))\n",
    "    \n",
    "    return cnn_model_three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfb1305",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_three = cnn_model_three()\n",
    "\n",
    "# set up model optimizer, loss function, and accuracy\n",
    "#adam - adaptive moment estimation\n",
    "cnn_model_three.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "cnn_model_three.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c88103",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "t0_three = time.time()\n",
    "train_model_three = cnn_model_three.fit(train_X, train_y, epochs=10, validation_split=0.33)\n",
    "print(\"Training time:\", time.time()-t0_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152a8b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model\n",
    "loss, acc = cnn_model_three.evaluate(test_X, test_y, verbose = 1)\n",
    "print('\\nTest accuracy: ', acc)\n",
    "print('\\nTest loss: ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52130ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the accuracy\n",
    "plt.plot(train_model_three.history['sparse_categorical_accuracy'])\n",
    "plt.plot(train_model_three.history['val_sparse_categorical_accuracy'])\n",
    "plt.title('Model Accuracy - CNN Three')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b290b6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the loss\n",
    "plt.plot(train_model_three.history['loss'])\n",
    "plt.plot(train_model_three.history['val_loss'])\n",
    "plt.title('Model Loss - CNN Three')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6823c817",
   "metadata": {},
   "source": [
    "## CNN_Model_Four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803a1dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_four():\n",
    "    \n",
    "    #creating a sequential instance to add layers to model\n",
    "    cnn_model_four = Sequential()\n",
    "    \n",
    "    #first convolutional layer\n",
    "    cnn_model_four.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(28,28,1)))\n",
    "    #add batch normalization\n",
    "    cnn_model_four.add(BatchNormalization())\n",
    "    #first pooling layer\n",
    "    cnn_model_four.add(MaxPooling2D((2,2), strides = 2))\n",
    "    \n",
    "    \n",
    "    #second convolutional layer\n",
    "    cnn_model_four.add(Conv2D(64, (3,3), padding='same', activation = 'relu'))\n",
    "    #add batch normalization\n",
    "    cnn_model_four.add(BatchNormalization())\n",
    "    #second pooling layer\n",
    "    cnn_model_four.add(MaxPooling2D((2,2), strides = 2))\n",
    "\n",
    "    #third convolutional layer\n",
    "    cnn_model_four.add(Conv2D(128, (3,3), padding='same', activation = 'relu'))\n",
    "    #add batch normalization\n",
    "    cnn_model_four.add(BatchNormalization())\n",
    "    #third pooling layer\n",
    "    cnn_model_four.add(MaxPooling2D((2,2), strides = 2))\n",
    "\n",
    "    #Flatten and connect layers\n",
    "    cnn_model_four.add(Flatten())\n",
    "    cnn_model_four.add(Dense(128, activation = 'relu'))\n",
    "    cnn_model_four.add(Dense(128, activation = 'relu'))\n",
    "    cnn_model_four.add(Dense(128, activation = 'relu'))\n",
    "    cnn_model_four.add(Dense(128, activation = 'relu'))\n",
    "    cnn_model_four.add(Dense(128, activation = 'relu'))\n",
    "    cnn_model_four.add(Dense(128, activation = 'relu'))\n",
    "    cnn_model_four.add(Dropout(0.5))\n",
    "    \n",
    "    #Fully connected layer of 10 to refelct 10 labels and softmax activation\n",
    "    cnn_model_four.add(Dense(10, activation = 'softmax'))\n",
    "    \n",
    "    return cnn_model_four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fb3e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_four = cnn_model_four()\n",
    "\n",
    "# set up model optimizer, loss function, and accuracy\n",
    "#adam - adaptive moment estimation\n",
    "cnn_model_four.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "cnn_model_four.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f690da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "t0_four = time.time()\n",
    "train_model_four = cnn_model_four.fit(train_X, train_y, epochs=10, validation_split=0.33)\n",
    "print(\"Training time:\", time.time()-t0_four)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171a7ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model\n",
    "loss, acc = cnn_model_four.evaluate(test_X, test_y, verbose = 1)\n",
    "print('\\nTest accuracy: ', acc)\n",
    "print('\\nTest loss: ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2f061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the accuracy\n",
    "plt.plot(train_model_four.history['sparse_categorical_accuracy'])\n",
    "plt.plot(train_model_four.history['val_sparse_categorical_accuracy'])\n",
    "plt.title('Model Accuracy - CNN Four')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfa80ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the loss\n",
    "plt.plot(train_model_four.history['loss'])\n",
    "plt.plot(train_model_four.history['val_loss'])\n",
    "plt.title('Model Loss - CNN Four')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
