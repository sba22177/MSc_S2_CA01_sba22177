{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12a8699e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-03 20:58:34.765911: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-03 20:58:34.766009: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-03 20:58:34.771291: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-03 20:58:35.389132: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-03 20:58:39.668796: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "import pyspark\n",
    "import tensorframes\n",
    "import pyspark.sql.functions as f\n",
    "import sparkdl as dl\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, BooleanType\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType, ArrayType\n",
    "from keras.datasets import fashion_mnist\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50242144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 22:23:55,564 WARN util.Utils: Your hostname, fiona-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "2023-10-02 22:23:55,567 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.2.4-bin-hadoop3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/hduser/.ivy2/cache\n",
      "The jars for the packages stored in: /home/hduser/.ivy2/jars\n",
      "databricks#spark-deep-learning added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8e58085c-31ea-493c-902a-35085abcfaa0;1.0\n",
      "\tconfs: [default]\n",
      ":: resolution report :: resolve 4017ms :: artifacts dl 1ms\n",
      "\t:: modules in use:\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   0   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: WARNINGS\n",
      "\t\tmodule not found: databricks#spark-deep-learning;0.1.0-spark2.1-s_2.11.\n",
      "\n",
      "\t==== local-m2-cache: tried\n",
      "\n",
      "\t  file:/home/hduser/.m2/repository/databricks/spark-deep-learning/0.1.0-spark2.1-s_2.11./spark-deep-learning-0.1.0-spark2.1-s_2.11..pom\n",
      "\n",
      "\t  -- artifact databricks#spark-deep-learning;0.1.0-spark2.1-s_2.11.!spark-deep-learning.jar:\n",
      "\n",
      "\t  file:/home/hduser/.m2/repository/databricks/spark-deep-learning/0.1.0-spark2.1-s_2.11./spark-deep-learning-0.1.0-spark2.1-s_2.11..jar\n",
      "\n",
      "\t==== local-ivy-cache: tried\n",
      "\n",
      "\t  /home/hduser/.ivy2/local/databricks/spark-deep-learning/0.1.0-spark2.1-s_2.11./ivys/ivy.xml\n",
      "\n",
      "\t  -- artifact databricks#spark-deep-learning;0.1.0-spark2.1-s_2.11.!spark-deep-learning.jar:\n",
      "\n",
      "\t  /home/hduser/.ivy2/local/databricks/spark-deep-learning/0.1.0-spark2.1-s_2.11./jars/spark-deep-learning.jar\n",
      "\n",
      "\t==== central: tried\n",
      "\n",
      "\t  https://repo1.maven.org/maven2/databricks/spark-deep-learning/0.1.0-spark2.1-s_2.11./spark-deep-learning-0.1.0-spark2.1-s_2.11..pom\n",
      "\n",
      "\t  -- artifact databricks#spark-deep-learning;0.1.0-spark2.1-s_2.11.!spark-deep-learning.jar:\n",
      "\n",
      "\t  https://repo1.maven.org/maven2/databricks/spark-deep-learning/0.1.0-spark2.1-s_2.11./spark-deep-learning-0.1.0-spark2.1-s_2.11..jar\n",
      "\n",
      "\t==== spark-packages: tried\n",
      "\n",
      "\t  https://repos.spark-packages.org/databricks/spark-deep-learning/0.1.0-spark2.1-s_2.11./spark-deep-learning-0.1.0-spark2.1-s_2.11..pom\n",
      "\n",
      "\t  -- artifact databricks#spark-deep-learning;0.1.0-spark2.1-s_2.11.!spark-deep-learning.jar:\n",
      "\n",
      "\t  https://repos.spark-packages.org/databricks/spark-deep-learning/0.1.0-spark2.1-s_2.11./spark-deep-learning-0.1.0-spark2.1-s_2.11..jar\n",
      "\n",
      "\t\t::::::::::::::::::::::::::::::::::::::::::::::\n",
      "\n",
      "\t\t::          UNRESOLVED DEPENDENCIES         ::\n",
      "\n",
      "\t\t::::::::::::::::::::::::::::::::::::::::::::::\n",
      "\n",
      "\t\t:: databricks#spark-deep-learning;0.1.0-spark2.1-s_2.11.: not found\n",
      "\n",
      "\t\t::::::::::::::::::::::::::::::::::::::::::::::\n",
      "\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      "Exception in thread \"main\" java.lang.RuntimeException: [unresolved dependency: databricks#spark-deep-learning;0.1.0-spark2.1-s_2.11.: not found]\n",
      "\tat org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:1458)\n",
      "\tat org.apache.spark.util.DependencyUtils$.resolveMavenDependencies(DependencyUtils.scala:185)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:319)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:909)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:191)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:214)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1054)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1063)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2950/2963260687.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sba22177_Integrated_CA01_2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                         \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "#initalizing spark session\n",
    "spark = SparkSession.builder \\\n",
    ".appName(\"sba22177_Integrated_CA01_2\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbad5142",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dowload the MNIST data\n",
    "(train_X, train_y), (test_X, test_y) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0da4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#examining dataset\n",
    "print(test_X.shape)\n",
    "print(train_X.shape)\n",
    "print(test_y.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43de635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing unique labels\n",
    "set(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc03631",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the image classes so not just numbers as above\n",
    "class_names = ['T-shirt/top','Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e665a330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature scaling colours from 0-255 to 0-1\n",
    "train_X = train_X/255.0\n",
    "test_X = test_X/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637405a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing the first 10 images\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(10):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_X[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[train_y[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbb0ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_df = train_X.reshape((train_X.shape[0], -1))\n",
    "test_X_df = test_X.reshape((test_X.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a775c2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#examining dataset\n",
    "print(test_X_df.shape)\n",
    "print(train_X_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33040de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_code=1\n",
    "test_code=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eecc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=[\n",
    "    (i,\n",
    "    train_X_df[i].astype(int).tolist(),\n",
    "    int(train_y[i]),\n",
    "    train_code,\n",
    "    )  for i in range(len(train_y))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fc33ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=[\n",
    "    (i,\n",
    "    test_X_df[i].astype(int).tolist(),\n",
    "    int(test_y[i]),\n",
    "    test_code,\n",
    "    )  for i in range(len(test_y))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6f3e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"features\", ArrayType(IntegerType()), True),\n",
    "    StructField(\"label\", IntegerType(), True),\n",
    "    StructField(\"code\", IntegerType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc2ba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = spark.createDataFrame(train_data, schema=schema)\n",
    "test_df = spark.createDataFrame(test_data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa6d38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af158a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e5b4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = train_df.union(test_df)\n",
    "df_concat.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee4c3fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5f3c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_vector_to_list = udf(lambda v: list(v), ArrayType(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a934df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.withColumn(\"features\", dense_vector_to_list(train_df[\"features\"]))\n",
    "test_df = test_df.withColumn(\"features\", dense_vector_to_list(test_df[\"features\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6491e897",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.array(train_df.select(\"features\").collect(),dtype=np.float32)\n",
    "train_labels = np.array(train_df.select(\"label\").collect(),dtype=np.int32)\n",
    "test_features = np.array(train_df.select(\"features\").collect(),dtype=np.float32)\n",
    "test_labels = np.array(train_df.select(\"label\").collect(),dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed191fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset= tf.data.Dataset.from_tensor_slices((train_features, train_labels))\n",
    "train_dataset=train_dataset.shuffle(buffer_size=len(train_features)).batch(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51127e95",
   "metadata": {},
   "source": [
    "## CNN_Model_One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2d03cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_one():\n",
    "    \n",
    "    #creating a sequential instance to add layers to model\n",
    "    cnn_model = Sequential()\n",
    "    \n",
    "    #first convolutional layer\n",
    "    cnn_model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(28,28,1)))\n",
    "    #first pooling layer\n",
    "    cnn_model.add(MaxPooling2D((2,2), strides = 2))\n",
    "    \n",
    "    #second convolutional layer\n",
    "    cnn_model.add(Conv2D(64, (3,3), padding='same', activation = 'relu'))\n",
    "    #second pooling layer\n",
    "    cnn_model.add(MaxPooling2D((2,2), strides = 2))\n",
    "    \n",
    "    #third convolutional layer\n",
    "    cnn_model.add(Conv2D(128, (3,3), padding='same', activation = 'relu'))\n",
    "    #third pooling layer\n",
    "    cnn_model.add(MaxPooling2D((2,2), strides = 2))\n",
    "    \n",
    "    #Flatten and connect layers\n",
    "    cnn_model.add(Flatten())\n",
    "    cnn_model.add(Dense(128, activation = 'relu'))\n",
    "    \n",
    "    #Fully connected layer of 10 to refelct 10 labels and softmax activation\n",
    "    cnn_model.add(Dense(10, activation = 'softmax'))\n",
    "    \n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de434aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_one = cnn_model_one()\n",
    "\n",
    "# set up model optimizer, loss function, and accuracy\n",
    "#adam - adaptive moment estimation\n",
    "cnn_model_one.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "cnn_model_one.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a2e53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "t0_one=time.time()\n",
    "train_model_one = cnn_model_one.fit(train_X, train_y, epochs=10, validation_split=0.33)\n",
    "print(\"Training time:\", time.time()-t0_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78e7565",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model\n",
    "loss, acc = cnn_model_one.evaluate(test_X, test_y, verbose = 1)\n",
    "print('\\nTest accuracy: ', acc)\n",
    "print('\\nTest loss: ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9e6d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the accuracy\n",
    "plt.plot(train_model_one.history['sparse_categorical_accuracy'])\n",
    "plt.plot(train_model_one.history['val_sparse_categorical_accuracy'])\n",
    "plt.title('Model Accuracy - CNN One')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671bc224",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the loss\n",
    "plt.plot(train_model_one.history['loss'])\n",
    "plt.plot(train_model_one.history['val_loss'])\n",
    "plt.title('Model Loss - CNN One')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b08c47",
   "metadata": {},
   "source": [
    "## CNN_Model_One - Epoch Increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f530b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "t0_one_epoch.time()\n",
    "train_model_one_epoch = cnn_model_one.fit(train_X, train_y, epochs=30, validation_split=0.33)\n",
    "print(\"Training time:\", time.time()-t0_one_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486dea25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model\n",
    "loss, acc = cnn_model_one.evaluate(test_X, test_y, verbose = 1)\n",
    "print('\\nTest accuracy: ', acc)\n",
    "print('\\nTest loss: ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a920a7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the accuracy\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.plot(train_model_one_epoch.history['sparse_categorical_accuracy'], label='Train')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('Train')\n",
    "ax1.tick_params(axis='y')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.plot(train_model_one_epoch.history['val_sparse_categorical_accuracy'], label='Validation', color='orange')\n",
    "ax2.set_ylabel('Validation')\n",
    "ax2.tick_params(axis='y')\n",
    "\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Model Accuracy - CNN One Epoch Increase')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f542b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the loss\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.plot(train_model_one_epoch.history['loss'], label='Train')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('Train')\n",
    "ax1.tick_params(axis='y')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.plot(train_model_one_epoch.history['val_loss'], label='Validation', color='orange')\n",
    "ax2.set_ylabel('Validation')\n",
    "ax2.tick_params(axis='y')\n",
    "\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Model Loss - CNN One Epoch Increase')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80385cc",
   "metadata": {},
   "source": [
    "## CNN_Model_Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd425f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_two():\n",
    "    \n",
    "    #creating a sequential instance to add layers to model\n",
    "    cnn_model_two = Sequential()\n",
    "    \n",
    "    #first convolutional layer\n",
    "    cnn_model_two.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(28,28,1)))\n",
    "    #first pooling layer\n",
    "    cnn_model_two.add(MaxPooling2D((2,2), strides = 2))\n",
    "    \n",
    "    #second convolutional layer\n",
    "    cnn_model_two.add(Conv2D(64, (3,3), padding='same', activation = 'relu'))\n",
    "    #second pooling layer\n",
    "    cnn_model_two.add(MaxPooling2D((2,2), strides = 2))\n",
    "    \n",
    "    #third convolutional layer\n",
    "    cnn_model_two.add(Conv2D(128, (3,3), padding='same', activation = 'relu'))\n",
    "    #third pooling layer\n",
    "    cnn_model_two.add(MaxPooling2D((2,2), strides = 2))\n",
    "    \n",
    "    #Flatten and connect layers\n",
    "    cnn_model_two.add(Flatten())\n",
    "    cnn_model_two.add(Dense(128, activation = 'relu'))\n",
    "    cnn_model_two.add(Dense(128, activation = 'relu'))\n",
    "    cnn_model_two.add(Dense(128, activation = 'relu'))\n",
    "    cnn_model_two.add(Dense(128, activation = 'relu'))\n",
    "    cnn_model_two.add(Dense(128, activation = 'relu'))\n",
    "    cnn_model_two.add(Dense(128, activation = 'relu'))\n",
    "    \n",
    "    #Fully connected layer of 10 to refelct 10 labels and softmax activation\n",
    "    cnn_model_two.add(Dense(10, activation = 'softmax'))\n",
    "    \n",
    "    return cnn_model_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb72813",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_two = cnn_model_two()\n",
    "\n",
    "# set up model optimizer, loss function, and accuracy\n",
    "#adam - adaptive moment estimation\n",
    "cnn_model_two.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "cnn_model_two.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b5b15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "t0_two=time.time()\n",
    "train_model_two = cnn_model_two.fit(train_X, train_y, epochs=10, validation_split=0.33)\n",
    "print(\"Training time:\", time.time()-t0_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b248c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model\n",
    "loss, acc = cnn_model_two.evaluate(test_X, test_y, verbose = 1)\n",
    "print('\\nTest accuracy: ', acc)\n",
    "print('\\nTest loss: ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fd0425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the accuracy\n",
    "plt.plot(train_model_two.history['sparse_categorical_accuracy'])\n",
    "plt.plot(train_model_two.history['val_sparse_categorical_accuracy'])\n",
    "plt.title('Model Accuracy - CNN Two')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03590f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the loss\n",
    "plt.plot(train_model_two.history['loss'])\n",
    "plt.plot(train_model_two.history['val_loss'])\n",
    "plt.title('Model Loss - CNN Two')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963f0283",
   "metadata": {},
   "source": [
    "## CNN_MODEL_THREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c49280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_three():\n",
    "    \n",
    "    #creating a sequential instance to add layers to model\n",
    "    cnn_model_three = Sequential()\n",
    "    \n",
    "    #first convolutional layer\n",
    "    cnn_model_three.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(28,28,1)))\n",
    "    #first pooling layer\n",
    "    cnn_model_three.add(MaxPooling2D((2,2), strides = 2))\n",
    "    #add dropout\n",
    "    model.add(Dropout(0.3)) \n",
    "    \n",
    "    #second convolutional layer\n",
    "    cnn_model_three.add(Conv2D(64, (3,3), padding='same', activation = 'relu'))\n",
    "    #second pooling layer\n",
    "    cnn_model_three.add(MaxPooling2D((2,2), strides = 2))\n",
    "    #add dropout\n",
    "    model.add(Dropout(0.3)) \n",
    "    \n",
    "    #third convolutional layer\n",
    "    cnn_model_three.add(Conv2D(128, (3,3), padding='same', activation = 'relu'))\n",
    "    #third pooling layer\n",
    "    cnn_model_three.add(MaxPooling2D((2,2), strides = 2))\n",
    "    #add dropout\n",
    "    model.add(Dropout(0.3)) \n",
    "    \n",
    "    #Flatten and connect layers\n",
    "    cnn_model_three.add(Flatten())\n",
    "    cnn_model_three.add(Dense(128, activation = 'relu'))\n",
    "    cnn_model_three.add(Dense(128, activation = 'relu'))\n",
    "    cnn_model_three.add(Dense(128, activation = 'relu'))\n",
    "    cnn_model_three.add(Dense(128, activation = 'relu'))\n",
    "    cnn_model_three.add(Dense(128, activation = 'relu'))\n",
    "    cnn_model_three.add(Dense(128, activation = 'relu'))\n",
    "    model.add(Dropout(0.3)) \n",
    "    \n",
    "    #Fully connected layer of 10 to refelct 10 labels and softmax activation\n",
    "    cnn_model_three.add(Dense(10, activation = 'softmax'))\n",
    "    \n",
    "    return cnn_model_three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b7f10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_three = cnn_model_three()\n",
    "\n",
    "# set up model optimizer, loss function, and accuracy\n",
    "#adam - adaptive moment estimation\n",
    "cnn_model_three.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "cnn_model_three.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdc2759",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "t0_three = time.time()\n",
    "train_model_three = cnn_model_three.fit(train_X, train_y, epochs=10, validation_split=0.33)\n",
    "print(\"Training time:\", time.time()-t0_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad9f8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model\n",
    "loss, acc = cnn_model_three.evaluate(test_X, test_y, verbose = 1)\n",
    "print('\\nTest accuracy: ', acc)\n",
    "print('\\nTest loss: ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2e0775",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the accuracy\n",
    "plt.plot(train_model_three.history['sparse_categorical_accuracy'])\n",
    "plt.plot(train_model_three.history['val_sparse_categorical_accuracy'])\n",
    "plt.title('Model Accuracy - CNN Three')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1914f387",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the loss\n",
    "plt.plot(train_model_three.history['loss'])\n",
    "plt.plot(train_model_three.history['val_loss'])\n",
    "plt.title('Model Loss - CNN Three')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16767f0b",
   "metadata": {},
   "source": [
    "## CNN_Model_Four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eb9a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_four():\n",
    "    \n",
    "    #creating a sequential instance to add layers to model\n",
    "    cnn_model_four = Sequential()\n",
    "    \n",
    "    #first convolutional layer\n",
    "    cnn_model_four.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(28,28,1)))\n",
    "    #add batch normalization\n",
    "    cnn_model_four.add(BatchNormalization())\n",
    "    #first pooling layer\n",
    "    cnn_model_four.add(MaxPooling2D((2,2), strides = 2))\n",
    "    \n",
    "    \n",
    "    #second convolutional layer\n",
    "    cnn_model_four.add(Conv2D(64, (3,3), padding='same', activation = 'relu'))\n",
    "    #add batch normalization\n",
    "    cnn_model_four.add(BatchNormalization())\n",
    "    #second pooling layer\n",
    "    cnn_model_four.add(MaxPooling2D((2,2), strides = 2))\n",
    "\n",
    "    #third convolutional layer\n",
    "    cnn_model_four.add(Conv2D(128, (3,3), padding='same', activation = 'relu'))\n",
    "    #add batch normalization\n",
    "    cnn_model_four.add(BatchNormalization())\n",
    "    #third pooling layer\n",
    "    cnn_model_four.add(MaxPooling2D((2,2), strides = 2))\n",
    "\n",
    "    #Flatten and connect layers\n",
    "    cnn_model_four.add(Flatten())\n",
    "    cnn_model_four.add(Dense(128, activation = 'relu'))\n",
    "    cnn_model_four.add(Dense(128, activation = 'relu'))\n",
    "    cnn_model_four.add(Dense(128, activation = 'relu'))\n",
    "    cnn_model_four.add(Dense(128, activation = 'relu'))\n",
    "    cnn_model_four.add(Dense(128, activation = 'relu'))\n",
    "    cnn_model_four.add(Dense(128, activation = 'relu'))\n",
    "    cnn_model_four.add(Dropout(0.5))\n",
    "    \n",
    "    #Fully connected layer of 10 to refelct 10 labels and softmax activation\n",
    "    cnn_model_four.add(Dense(10, activation = 'softmax'))\n",
    "    \n",
    "    return cnn_model_four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5e3128",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_four = cnn_model_four()\n",
    "\n",
    "# set up model optimizer, loss function, and accuracy\n",
    "#adam - adaptive moment estimation\n",
    "cnn_model_four.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "cnn_model_four.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4738671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "t0_four = time.time()\n",
    "train_model_four = cnn_model_four.fit(train_X, train_y, epochs=10, validation_split=0.33)\n",
    "print(\"Training time:\", time.time()-t0_four)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7c3758",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model\n",
    "loss, acc = cnn_model_four.evaluate(test_X, test_y, verbose = 1)\n",
    "print('\\nTest accuracy: ', acc)\n",
    "print('\\nTest loss: ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14546d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the accuracy\n",
    "plt.plot(train_model_four.history['sparse_categorical_accuracy'])\n",
    "plt.plot(train_model_four.history['val_sparse_categorical_accuracy'])\n",
    "plt.title('Model Accuracy - CNN Four')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0ec745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the loss\n",
    "plt.plot(train_model_four.history['loss'])\n",
    "plt.plot(train_model_four.history['val_loss'])\n",
    "plt.title('Model Loss - CNN Four')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769acd27",
   "metadata": {},
   "source": [
    "## CNN_Model_Five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b05774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_four():\n",
    "    \n",
    "    #creating a sequential instance to add layers to model\n",
    "    cnn_model_four = Sequential()\n",
    "    \n",
    "    #first convolutional layer\n",
    "    cnn_model_four.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(28,28,1)))\n",
    "    cnn_model_four.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(28,28,1)))\n",
    "    cnn_model_four.add(MaxPooling2D((2,2), strides = 2))\n",
    "    cnn_model_four.add(Dropout(0.2))\n",
    "    cnn_model_four.add(Conv2D(64, (3,3), padding='same', activation = 'relu'))\n",
    "    cnn_model_four.add(Conv2D(64, (3,3), padding='same', activation = 'relu'))\n",
    "    cnn_model_four.add(BatchNormalization())\n",
    "    cnn_model_four.add(MaxPooling2D((2,2), strides = 2))\n",
    "    cnn_model_four.add(Dropout(0.3))\n",
    "\n",
    "    #Flatten and connect layers\n",
    "    cnn_model_four.add(Flatten())\n",
    "    cnn_model_four.add(Dense(256, activation = 'relu'))\n",
    "    cnn_model_four.add(BatchNormalization())\n",
    "    cnn_model_four.add(Dropout(0.5))\n",
    "    cnn_model_four.add(Dropout(0.7))\n",
    "\n",
    "    \n",
    "    #Fully connected layer of 10 to refelct 10 labels and softmax activation\n",
    "    cnn_model_four.add(Dense(10, activation = 'softmax'))\n",
    "    \n",
    "    return cnn_model_four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268fb2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_four = cnn_model_four()\n",
    "\n",
    "# set up model optimizer, loss function, and accuracy\n",
    "#adam - adaptive moment estimation\n",
    "cnn_model_four.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "cnn_model_four.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ad7ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "t0_four = time.time()\n",
    "train_model_four = cnn_model_four.fit(train_X, train_y, epochs=10, validation_split=0.33)\n",
    "print(\"Training time:\", time.time()-t0_four)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6410fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model\n",
    "loss, acc = cnn_model_four.evaluate(test_X, test_y, verbose = 1)\n",
    "print('\\nTest accuracy: ', acc)\n",
    "print('\\nTest loss: ', loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
